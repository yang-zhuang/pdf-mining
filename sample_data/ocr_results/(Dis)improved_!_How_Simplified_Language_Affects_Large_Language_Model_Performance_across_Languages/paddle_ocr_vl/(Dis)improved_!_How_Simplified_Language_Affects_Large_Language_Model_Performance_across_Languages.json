{
  "source_file": "D:\\papers\\ACL-2025-paper\\ACL\\2025\\(Dis)improved_!_How_Simplified_Language_Affects_Large_Language_Model_Performance_across_Languages.pdf",
  "total_pages": 15,
  "pages": [
    [
      {
        "label": "fig",
        "figure_path": "imgs/img_in_image_box_618_440_1040_648.jpg"
      },
      {
        "page_index": 0,
        "page_content": "# (Dis)improved?! How Simplified Language Affects Large Language Model Performance across Languages\n\nMiriam Anschütz, Anastasiya Damaratskaya, Chaeeun Joy Lee, Arthur Schmalz, Edoardo Mosca and Georg Groh\n\nTechnical University of Munich\n\nmiriam.anschuetz@tum.de, grohg@cit.tum.de\n\n## Abstract\n\nSimplified language enhances the accessibility and human understanding of texts. However, whether it also benefits large language models (LLMs) remains underexplored. This paper extensively studies whether LLM performance improves on simplified data compared to its original counterpart. Our experiments span six datasets and nine automatic simplification systems across three languages. We show that English models, including GPT-4o-mini, show a weak generalization and exhibit a significant performance drop on simplified data. This introduces an intriguing paradox: simplified data is helpful for humans but not for LLMs.\n\nAt the same time, the performance in non-English languages sometimes improves, depending on the task and quality of the simplifier. Our findings offer a comprehensive view of the impact of simplified language on LLM performance and uncover severe implications for people depending on simple language.\n\n## 1 Introduction\n\nAutomatic Text Simplification (ATS) is the task of rewriting a text using simpler vocabulary while preserving its original meaning. The goal is to increase readability and make information accessible to a broader audience. The primary target group is people with low literacy and mental disabilities, or language learners (Martin et al., 2022). However, previous work has shown that not only people from the target group but even the broad majority of people profit from simplified language (Javourey-Drevet et al., 2022; Murphy Odo, 2022). With this paper, we try to answer if the same holds true for Large Language Models (LLMs). Given that LLMs are approaching human-like capabilities (Grattafiori et al., 2024), it is reasonable to hypothesize that they might also perform better with simplified input or at least show good performance and generalization on this language style.\n\n<div style=\"text-align: center;\"><img src=\"imgs/img_in_image_box_618_440_1040_648.jpg\" alt=\"Image\" width=\"35%\" /></div>\n\n\n<div style=\"text-align: center;\">Figure 1: Text sample from the Sentiment Analysis for Financial News dataset (Malo et al., 2014). We test the generalization of LLMs like Llama3.1 70B from original to automatically simplified data. The sentiment prediction on the original data sample is correct. However, if we use an automatic lexical simplifier that replaced the word “operates” with “works”, Llama misclassifies the sample as positive.</div>\n\n\nTo investigate this, we select six labeled datasets across three languages (English, German, and Russian) and simplify their texts using nine pretrained simplification models and LLMs. Then, we benchmark five large language models, including Llama3.1 (Grattafiori et al., 2024), Aya Expanse (Dang et al., 2024), and GPT-4o-mini, on both the original and simplified corpora. Our results show a significant change in performance with a strong performance drop for English (see example in Figure 1). This lack of generalization introduces a severe risk for people who rely on simplified language: If they input prompts or samples in simple language, LLMs may show a worse performance and make more mistakes than with standard English. Especially for tasks with high societal impact, like fake news classification or news summarization, this increases discrimination for already vulnerable target groups.\n\nOverall, our contributions can be summarized as follows:\n\n• We present a large-scale multilingual benchmark of LLM generalization on simplified data, including s.o.t.a. models like Llama3.1,"
      }
    ],
    [
      {
        "page_index": 1,
        "page_content": "Aya Expanse, and GPT-4o-mini. The simplifications are evaluated on a broad range of metrics, covering readability and meaning preservation, and a human review.\n\n• Our results indicate a significant performance decline on English simplified data, but with promising improvements in non-English languages.\n\n• All code, simplified data, and model predictions are publicly available for further investigation and experimentation $ ^{1} $ .\n\n## 2 Related work\n\nThe impact of ATS on NLP tasks has been studied for many years and for different NLP tasks (Vickrey and Koller, 2008; Schmidek and Barbosa, 2014; Štajner and Popovic, 2016). However, many of the older studies could not use transformers or even large language models and were based on statistical simplification. Among the more recent studies, we identify two research directions: text simplification as data augmentation for pre-training or fine-tuning and text simplification as a pre-processing step to improve inference performance. To investigate the first direction, Van et al. (2021) simplified the training data for LSTM- and BERT-based classification models and evaluated the simplification quality with BLEU only. Results show that different setups of data augmentation with simplification can improve the classifiers. However, they also show that simplifying the data at inference time results in a weaker performance than the original data.\n\nThese results are in contrast to other studies that benchmarked simplification as inference preprocessing. Miyata and Tatsumi (2019) tested Google Translator for Japanese to English translations with sentence splitting and further rule-based simplifications. A human evaluation showed that the simplifications yielded strong improvements in the translation outputs. Similarly, Mehta et al. (2020) created an artificial simplification system through back translation and used this system to simplify the machine translation inputs of a low-resource-language translation system. They show improved translation quality across multiple languages. However, the performance changes of the target systems depend on the quality of the ATS systems. As such, Agrawal and Carpuat (2024) investigated how well ATS systems preserve the meaning of the original texts. While human simplifications could improve the performance of a pre-trained question-answering model, automatic simplifications worsened the performance. Our work tries to shed light on the contradicting findings of previous work. For this, we extend the existing research by covering more tasks, languages, and simplifiers. We paint a broader picture of the helpfulness of simplification as pre-processing, especially in times of flexible and powerful LLMs.\n\n\n\nA different research direction was chosen by Anschütz et al. (2024), who used human-supervised simplification corpora to investigate how well models generalize between original and simplified data. They are the first ones to include LLMs in their investigations and show that models exhibit an incoherent behavior between original and simplified data. However, they only benchmarked GPT3.5-turbo as LLM, and their datasets do not contain ground-truth labels. While they assumed that the human-supervised datasets contain correct simplifications, they cannot measure the actual performance of the classification system without ground-truth labels. We try to overcome this weakness by using labeled datasets and benchmarking the performance of multiple LLMs on these datasets. In addition, we extend the investigation to the task of summarization and not only cover classification tasks.\n\n## 3 Methodology\n\nOur objective is to compare whether the performance of different LLMs changes when the input samples are simplified. For this, we take labeled datasets and simplify the inputs with existing simplifiers. Then, we use pre-trained classification models or LLMs to predict the labels on the original and on the simplified inputs. Finally, we calculate the accuracy and examine whether text simplification at inference can improve the models' performance. An overview of our approach is shown in Figure 2. Our investigations cover three distinct languages with six different datasets, nine simplifiers, and six prediction models, including LLMs like GPT4o. All combinations were evaluated independently, and the models did not know if the input text was simplified or not to avoid bias. The different settings will be discussed in the following subsections."
      }
    ],
    [
      {
        "label": "fig",
        "figure_path": "imgs/img_in_image_box_147_139_566_401.jpg"
      },
      {
        "page_index": 2,
        "page_content": "<div style=\"text-align: center;\"><img src=\"imgs/img_in_image_box_147_139_566_401.jpg\" alt=\"Image\" width=\"35%\" /></div>\n\n\n<div style=\"text-align: center;\">Figure 2: Structure of our investigations. We compare the performance of the same model between the original inputs and their simplified versions. Red boxes indicate that these parts are investigated under different settings.</div>\n\n\n### 3.1 Datasets and tasks\n\nWe cover the tasks of classification and summarization. The evaluation of text generation is non-trivial since nuances of text and language characteristics need to be covered. In contrast, comparing classification labels is independent of the chosen metric. In addition, ATS systems may struggle to preserve the exact meaning (Säuberli et al., 2024; Agrawal and Carpuat, 2024). Classification tasks like reading comprehension and natural language inference focus on specific text details that can get lost during simplification (Trienes et al., 2024), even though the simplification is of high overall quality. To avoid depending on these details, we focus on more content-related tasks like topic and sentiment prediction. We assume that even if the simplifiers remove minor aspects, the overall content should not change significantly, and thus, the ground-truth labels are still correct for the simplified samples.\n\nThe selected datasets are shown in Table 1. We experiment with data in English, German, and Russian. All datasets are from the news domain, a general-purpose domain often targeted by ATS literature (Ryan et al., 2023). For each of the datasets, we only worked with the test splits. To reduce the financial efforts of the OpenAI API, we created fixed subsets of the AG News and the sentiment dataset and only used these subsets when prompting this API. In the following, results that are based on these subsets are indicated with  $ \\dagger $ . Each language contains a multi-task dataset that provides data for topic classification and summarization at the same time to enable a multi-task evaluation. The number of classes and granularity of the classes differ among the languages and tasks. The AG News dataset has four very general classes, while the TL;DR dataset focuses more on technical news and its subcategories. For the sentiment task, we purposefully selected a dataset with only three classes (positive, negative, and neutral) to avoid ambiguity. The summarization task is headline generation, where the models create a headline for the respective news snippet. This task has a strongly abstractive nature and is well-suited to evaluate how well the models can retrieve the most important information from the texts (Scialom et al., 2020).\n\n\n\n### 3.2 Simplifiers\n\nWe used nine different pre-trained simplification models for our experiments: two multilingual models for all languages and seven language-specific models (five for English, one for German, and one for Russian). Our model selection was limited by the availability and reproducibility of existing approaches. Especially unmaintained or weakly-documented Github repositories make reusing pretrained models challenging (Stodden, 2024; Kew et al., 2023). Nevertheless, the models that we could run give a good variety of approaches, ranging from lexical to paragraph-level simplification, and are trained for general-purpose or specialized domains. For all models, we used the default configurations provided in their repositories or model cards, and we did not add any further preprocessing. We used these simplification models:\n\nMILES (multiling.) is a lexical simplification pipeline. It uses frequency-based complex word identification and replaces the complex words with a lexical simplifier similar to LSBert (Qiang et al., 2020). It is available in 22 languages, including our investigated languages.\n\nDISSIM (EN) (Niklaus et al., 2019) is a rule-based syntactic simplification framework. We use it as a controllable baseline. Unfortunately, although claimed otherwise in the original paper, the published code only works on English data.\n\nGPT4o mini (multiling.) is one of the state-of-the-art LLMs by OpenAI and offers support for all three languages. We prompted it in a zero-shot manner to simplify the text samples. The simplification prompts are presented in Appendix B.\n\nMUSS (EN) stands for “Multilingual Unsupervised Sentence Simplification” and is one of the most popular pre-trained sentence simplification models (Martin et al., 2022). We used the pre-trained muss_en_mined checkpoint that utilizes the BART architecture (Lewis et al., 2020). Even"
      }
    ],
    [
      {
        "page_index": 3,
        "page_content": "\n<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Language</td><td style='text-align: center;'>Dataset</td><td style='text-align: center;'>Dataset name</td><td style='text-align: center;'>Prediction Task</td><td style='text-align: center;'>#samples (sub-set size)</td><td style='text-align: center;'>#classes</td></tr><tr><td rowspan=\"3\">EN</td><td style='text-align: center;'>AG News</td><td style='text-align: center;'>AG News (Zhang et al., 2015)</td><td style='text-align: center;'>topic</td><td style='text-align: center;'>7600 (760)</td><td style='text-align: center;'>4</td></tr><tr><td style='text-align: center;'>Sentiment</td><td style='text-align: center;'>Sentiment Analysis for Financial News (Malo et al., 2014)</td><td style='text-align: center;'>sentiment</td><td style='text-align: center;'>4846 (970)</td><td style='text-align: center;'>3</td></tr><tr><td style='text-align: center;'>TL;DR</td><td style='text-align: center;'>tldr_news</td><td style='text-align: center;'>topic, summarization</td><td style='text-align: center;'>794</td><td style='text-align: center;'>5</td></tr><tr><td rowspan=\"2\">DE</td><td style='text-align: center;'>Gnad10</td><td style='text-align: center;'>10k German News Articles Datasets (Schabus et al., 2017)</td><td style='text-align: center;'>topic</td><td style='text-align: center;'>1028</td><td style='text-align: center;'>9</td></tr><tr><td style='text-align: center;'>ML SUM</td><td style='text-align: center;'>Multilingual summarization (DE) (Scaliom et al., 2020)</td><td style='text-align: center;'>topic, summarization</td><td style='text-align: center;'>579</td><td style='text-align: center;'>12</td></tr><tr><td style='text-align: center;'>RU</td><td style='text-align: center;'>ML SUM</td><td style='text-align: center;'>Multilingual summarization (RU) (Scaliom et al., 2020)</td><td style='text-align: center;'>topic, summarization</td><td style='text-align: center;'>203</td><td style='text-align: center;'>9</td></tr></table>\n\n<div style=\"text-align: center;\">Table 1: Overview of all datasets and their classification tasks evaluated in this study.</div>\n\n\nthough MUSS is multilingual, it does not support all the languages we investigate. Due to the long runtime of MUSS, we create simplifications only on the fixed subsets of the data.\n\nCochrane and Medeasi (EN) are based on the HuggingFace space simplification-model-app. Both utilize a BART model fine-tuned for simplification in the medical domain. The Medeasi checkpoint uses the sentence-level MED-EASi dataset (Basu et al., 2023), while Cochrane is fine-tuned on the paragraph-level data (Devaraj et al., 2021).\n\nSimplifyText (EN) uses the Keep it Simple (KiS) approach by Laban et al. (2021) and is a GPT2-based simplification model.\n\nDEplain (DE) is a German simplification model based on mT5 (Stodden, 2024) and fine-tuned on the DEplain-APA corpus (Stodden et al., 2023).\n\nRussian simplification (RU) is a Russian sentence simplification model. It is based on ruT5 and was fine-tuned on the RuSimpleSentEval (Sakhovskiy et al., 2021) and the RuAdapt (Dmitrieva and Tiedemann, 2021) datasets.\n\n### 3.3 Classifiers and LLMs\n\nOur models under test span from DeBERTa-based classification systems to the latest open- and closed-source large language models. Table 2 gives an overview of the models and settings that we investigated.\n\nFor each English classification dataset, we fine-tuned two DeBERTaV3-base classifiers (He et al., 2023). The first classifier was trained on the original data, while the other classifier was fine-tuned on the data simplified with the SimplifyText model. We selected this model for simplification because it received the best scores among the open-source\n\n\n<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Model</td><td style='text-align: center;'>Setting</td><td style='text-align: center;'>Language(s)</td></tr><tr><td style='text-align: center;'>DeBERTaV3</td><td style='text-align: center;'>FT Orig</td><td style='text-align: center;'>EN</td></tr><tr><td style='text-align: center;'>DeBERTaV3</td><td style='text-align: center;'>FT Simple</td><td style='text-align: center;'>EN</td></tr><tr><td style='text-align: center;'>Llama3.1 8B Instruct</td><td style='text-align: center;'>Zero-shot</td><td style='text-align: center;'>EN, DE</td></tr><tr><td style='text-align: center;'>Llama3.1 70B Instruct</td><td style='text-align: center;'>Zero-shot</td><td style='text-align: center;'>EN, DE</td></tr><tr><td style='text-align: center;'>Aya Expanse 8B</td><td style='text-align: center;'>Zero-shot</td><td style='text-align: center;'>EN, DE, RU</td></tr><tr><td style='text-align: center;'>GPT-4o-mini</td><td style='text-align: center;'>Zero-shot</td><td style='text-align: center;'>EN, DE, RU</td></tr></table>\n\n<div style=\"text-align: center;\">Table 2: Overview of all models under test. Traditional models are fine-tuned on either the original training data or a simplified version of it. The LLMs are prompted in a zero-shot manner.</div>\n\n\nmodels in our unsupervised simplification evaluation (see subsection 3.4). Every training was conducted for one epoch with a learning rate of  $ 2 \\cdot 10^{-5} $ . We trained the models on the datasets' training splits, so the test splits used for our investigation were still unseen for the models. With this training setup, we can test how much the models adapt to the specific style of simplification and if text simplification as pre-processing or data augmentation during training is beneficial for performance.\n\nThe second part of our study investigated the performance of large language models. For this, we selected four LLMs, two open-source models from Meta's Llama3.1 family (Grattafiori et al., 2024) and Aya Expanse 8B from Cohere for AI (Dang et al., 2024), and the closed-source GPT4o-mini from OpenAI. Llama3.1 is a multilingual LLM with a context of 128k tokens. For our experiments, we use the instruction-tuned versions with 8B and 70B parameters to account for performance differences due to model size. Llama3.1 70B is loaded with bitsandbytes' 8-bit quantization. Unfortunately, Llama is not available in Russian. In contrast, Aya Expanse 8B exhibits powerful multilingual capacities and supports 23 languages, in"
      }
    ],
    [
      {
        "page_index": 4,
        "page_content": "including the three in our study. For GPT, we were limited to fixed subsets to reduce the financial efforts.\n\nFor the predictions themselves, we used the same zero-shot prompt for all four models. The prompts per dataset are presented in Appendix C. A native German or Russian speaker created each of the non-English prompts. Even if we told the models to only predict the topic and not provide any reasoning, some of the outputs still contained more content than the topic. We tried to account for the most common phrases among them during post-processing. Therefore, we lower-cased all model outputs and removed phrases like “The topic of this snippet is”. In addition, some labels were a combination of multiple terms, e.g., sci/tech in AG News. If only one part, e.g., only sci, was predicted, we considered this prediction correct and replaced it with the proper topic name.\n\n### 3.4 Unsupervised simplification evaluation\n\nPrevious work has investigated the impact of human-supervised simplifications (Anschütz et al., 2024), but for our datasets, human supervision is not feasible. In contrast, we investigate the impact of automatic text simplification, and thus, we need to evaluate the quality of the automatic simplifications. Our datasets are not targeted to simplification, and hence, no reference simplification exists. Therefore, we based our evaluation on unsupervised metrics that evaluate the simplification against the source instead of comparing it against a reference. While human evaluation would be the best solution, this is infeasible for our large-scale study setup with multiple languages, datasets, and simplifiers. To still provide an insightful evaluation of the simplifications, we not only evaluate the overall simplification quality but also the readability of the texts and the meaning preservation independently. To measure the readability of the texts and the simplicity-gain through simplification, we used the Flesch-Reading-Ease (FRE) (Flesch, 1948). It is a statistical measure based on the number of words per sentence and the average word length. It can be adapted for many languages, including German and Russian. The score ranges from 0 to 100, with a higher score indicating a higher readability. We used the Python textstat package and the German adaptation by Amstad (1978).\n\nThe second aspect of our evaluation is the overall simplification quality. For this, we use two different scores, which are LENS_SALSA (Heineman et al., 2023) and REFeREE (Huang and Kochmar, 2024). Both metrics are learned metrics that were fine-tuned to mimic human annotation scores. LENS_SALSA is working on the word-and sentence-level and predicts and scores edit annotations that are performed during simplification. In contrast to this, REFeREE employs a multi-step fine-tuning process that aligns the metric scores with traditional metrics like BLEU (Papineni et al., 2002) and performs a multi-aspect evaluation of the fluency and simplicity of the generated text. While LENS_SALSA ranges from 0 to 100, REFeREE only ranges from -1 to 1. Therefore, we rescale the REFeREE values to make them comparable with the other metrics.\n\n\n\nFinally, the third evaluation criterion is testing if the simplification preserves the original text's meaning. This is especially important for content classification tasks, as in our study. Again, we select two metrics to evaluate the factuality of the simplifications. First, we use FactCC (Kryscinski et al., 2020), which has shown the best human correlation on factuality evaluations like the FRANK dataset (Pagnoni et al., 2021). It was originally designed for the evaluation of abstractive summarization, but since some of our simplification systems perform complex operations close to summarization, we consider this metric suitable. FactCC employs a binary classification to predict whether the summary is factually consistent with its source. For our evaluation, we calculate the percentage of samples that are deemed correct to end up with a value between 0 and 100 again. The last metric is MeaningBERT (Beauchemin et al., 2023), which is specifically targeted toward meaning preservation in text simplification.\n\nWe provide a detailed evaluation and correlation analysis only for English, as FRE is the only unsupervised metric that we could find for German and Russian simplification.\n\n## 4 Results and Discussion\n\n### 4.1 Simplification evaluation\n\nWe evaluate the simplifications in English based on three criteria: the readability of the texts, the overall simplification quality, and the faithfulness of the simplifications. For this, we automatically score the simplifications with five different metrics (see subsection 3.4 for details). Table 3 shows the metrics scores for the English simplifications. DISSIM is a rule-based syntactical simplifier that,"
      }
    ],
    [
      {
        "page_index": 5,
        "page_content": "\n<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Metric</td><td style='text-align: center;'>Original</td><td style='text-align: center;'>DISSIM</td><td style='text-align: center;'>MILES</td><td style='text-align: center;'>Cochrane</td><td style='text-align: center;'>Medeasi</td><td style='text-align: center;'>SimplifyText</td><td style='text-align: center;'>MUSS</td><td style='text-align: center;'>GPT4o mini</td></tr><tr><td colspan=\"9\">AG News</td></tr><tr><td style='text-align: center;'>FRE</td><td style='text-align: center;'>48.78</td><td style='text-align: center;'>56.28  $ \\dagger $</td><td style='text-align: center;'>54.13</td><td style='text-align: center;'>70.22</td><td style='text-align: center;'>58.92</td><td style='text-align: center;'>65.93</td><td style='text-align: center;'>53.64  $ \\dagger $</td><td style='text-align: center;'>59.11  $ \\dagger $</td></tr><tr><td style='text-align: center;'>REFeREE</td><td style='text-align: center;'>-</td><td style='text-align: center;'>-7.17  $ \\dagger $</td><td style='text-align: center;'>36.08</td><td style='text-align: center;'>72.48</td><td style='text-align: center;'>67.19</td><td style='text-align: center;'>71.0</td><td style='text-align: center;'>65.35  $ \\dagger $</td><td style='text-align: center;'>87.84  $ \\dagger $</td></tr><tr><td style='text-align: center;'>LENS_SALSA</td><td style='text-align: center;'>-</td><td style='text-align: center;'>35.35  $ \\dagger $</td><td style='text-align: center;'>53.0</td><td style='text-align: center;'>66.56</td><td style='text-align: center;'>62.41</td><td style='text-align: center;'>64.66</td><td style='text-align: center;'>60.74  $ \\dagger $</td><td style='text-align: center;'>70.65  $ \\dagger $</td></tr><tr><td style='text-align: center;'>FactCC</td><td style='text-align: center;'>-</td><td style='text-align: center;'>86.58  $ \\dagger $</td><td style='text-align: center;'>91.63</td><td style='text-align: center;'>52.37</td><td style='text-align: center;'>85.04</td><td style='text-align: center;'>60.39</td><td style='text-align: center;'>84.87  $ \\dagger $</td><td style='text-align: center;'>85.53  $ \\dagger $</td></tr><tr><td style='text-align: center;'>Meaning_BERT</td><td style='text-align: center;'>-</td><td style='text-align: center;'>92.01  $ \\dagger $</td><td style='text-align: center;'>91.56</td><td style='text-align: center;'>67.41</td><td style='text-align: center;'>85.62</td><td style='text-align: center;'>83.29</td><td style='text-align: center;'>90.06  $ \\dagger $</td><td style='text-align: center;'>82.72  $ \\dagger $</td></tr><tr><td colspan=\"9\">Sentiment</td></tr><tr><td style='text-align: center;'>FRE</td><td style='text-align: center;'>55.43</td><td style='text-align: center;'>59.44  $ \\dagger $</td><td style='text-align: center;'>61.76</td><td style='text-align: center;'>73.34</td><td style='text-align: center;'>65.73</td><td style='text-align: center;'>65.52</td><td style='text-align: center;'>58.97  $ \\dagger $</td><td style='text-align: center;'>61.76  $ \\dagger $</td></tr><tr><td style='text-align: center;'>REFeREE</td><td style='text-align: center;'>-</td><td style='text-align: center;'>27.37  $ \\dagger $</td><td style='text-align: center;'>51.6</td><td style='text-align: center;'>56.74</td><td style='text-align: center;'>55.49</td><td style='text-align: center;'>67.59</td><td style='text-align: center;'>65.61  $ \\dagger $</td><td style='text-align: center;'>75.46  $ \\dagger $</td></tr><tr><td style='text-align: center;'>LENS_SALSA</td><td style='text-align: center;'>-</td><td style='text-align: center;'>50.7  $ \\dagger $</td><td style='text-align: center;'>60.34</td><td style='text-align: center;'>65.88</td><td style='text-align: center;'>56.42</td><td style='text-align: center;'>69.85</td><td style='text-align: center;'>64.29  $ \\dagger $</td><td style='text-align: center;'>69.34  $ \\dagger $</td></tr><tr><td style='text-align: center;'>FactCC</td><td style='text-align: center;'>-</td><td style='text-align: center;'>96.29  $ \\dagger $</td><td style='text-align: center;'>96.22</td><td style='text-align: center;'>54.5</td><td style='text-align: center;'>91.48</td><td style='text-align: center;'>73.85</td><td style='text-align: center;'>95.26  $ \\dagger $</td><td style='text-align: center;'>96.29  $ \\dagger $</td></tr><tr><td style='text-align: center;'>Meaning_BERT</td><td style='text-align: center;'>-</td><td style='text-align: center;'>90.36  $ \\dagger $</td><td style='text-align: center;'>84.84</td><td style='text-align: center;'>50.19</td><td style='text-align: center;'>85.12</td><td style='text-align: center;'>76.74</td><td style='text-align: center;'>83.27  $ \\dagger $</td><td style='text-align: center;'>78.68  $ \\dagger $</td></tr><tr><td colspan=\"9\">TL;DR</td></tr><tr><td style='text-align: center;'>FRE</td><td style='text-align: center;'>57.27</td><td style='text-align: center;'>56.45</td><td style='text-align: center;'>63.85</td><td style='text-align: center;'>76.2</td><td style='text-align: center;'>67.74</td><td style='text-align: center;'>62.08</td><td style='text-align: center;'>60.73</td><td style='text-align: center;'>62.32</td></tr><tr><td style='text-align: center;'>REFeREE</td><td style='text-align: center;'>-</td><td style='text-align: center;'>-12.58</td><td style='text-align: center;'>39.88</td><td style='text-align: center;'>75.25</td><td style='text-align: center;'>76.0</td><td style='text-align: center;'>79.93</td><td style='text-align: center;'>79.48</td><td style='text-align: center;'>84.64</td></tr><tr><td style='text-align: center;'>LENS_SALSA</td><td style='text-align: center;'>-</td><td style='text-align: center;'>36.88</td><td style='text-align: center;'>60.54</td><td style='text-align: center;'>72.05</td><td style='text-align: center;'>72.9</td><td style='text-align: center;'>73.95</td><td style='text-align: center;'>72.84</td><td style='text-align: center;'>75.74</td></tr><tr><td style='text-align: center;'>FactCC</td><td style='text-align: center;'>-</td><td style='text-align: center;'>89.29</td><td style='text-align: center;'>90.93</td><td style='text-align: center;'>49.75</td><td style='text-align: center;'>87.03</td><td style='text-align: center;'>66.37</td><td style='text-align: center;'>86.23</td><td style='text-align: center;'>88.92</td></tr><tr><td style='text-align: center;'>Meaning_BERT</td><td style='text-align: center;'>-</td><td style='text-align: center;'>91.25</td><td style='text-align: center;'>89.11</td><td style='text-align: center;'>67.89</td><td style='text-align: center;'>70.18</td><td style='text-align: center;'>84.22</td><td style='text-align: center;'>88.76</td><td style='text-align: center;'>87.77</td></tr></table>\n\n<div style=\"text-align: center;\">Table 3: Unsupervised simplification evaluation of the English simplifiers. For all metrics, higher scores indicate better simplification quality. The best scores per metric are bolded.  $ \\dagger $  evaluated only on subset</div>\n\n\nas expected, achieves a very high meaning preservation, but only small improvements in terms of readability and a poor overall simplification performance. The same is true about MILES, which, as a lexical simplification system, does not rewrite the sentences but only replaces some complex words within. In terms of readability, the Cochrane simplifier achieves the highest scores, indicating the biggest simplicity gain. Interestingly, the FRE scores of GPT4o-mini are rather low compared to the other simplifiers, indicating that it performs rather conservative simplification. Nevertheless, it achieves the best overall simplification quality across all datasets. This is probably due to its great fluency and overall capacities. In terms of faithfulness, MILES has the best scores among the LM-based simplifiers. This is expected since it is a lexical simplification system that does not rewrite the sentences but only replaces some complex words within. Overall, all simplification systems show a good performance and can be used for further experiments.\n\n### 4.2 Model performances\n\nTo investigate if the model performances change when we simplify the input texts, we compare the accuracies of all classification tasks and the rougeL scores (Lin, 2004) for the summarization tasks as implemented in Huggingface evaluate. For each dataset, we report the results of the two fine-tuned DeBERTa classifiers and the four LLMs in a zero-shot setting. In addition, we tested whether the changes in accuracy were statistically significant. For this, we performed a related t-test with the hypothesis that the average of the two distributions was the same. If the p-value is smaller than 0.05, we reject this hypothesis and can conclude that the accuracy change is significant. The results for the English tasks are presented in Table 4. A more detailed summarization analysis with further metrics beyond rougeL is provided in Appendix D. Overall, the fine-tuned classifiers (DeBERTa Orig and DeBERTa Simple) show the best accuracies, with GPT-4o-mini coming the closest.\n\n\n\nThe performance changes of the DISSIM syntactical baseline paint a mixed picture. We observe no statistically significant performance changes for the AG News dataset or the GPT4o-mini predictions. In contrast, for TL;DR data, the performance improves significantly, indicating that headline generation benefits from shorter sentences. Interestingly, Llama3.1 8B seems to benefit from that for some of the classification tasks as well. However, nearly all models show a decreased classification performance for end-to-end simplifications. Using these simplifiers, no performance improvement is statistically significant. However, the majority of the simplifications introduce a severe performance drop of up to 20 percentage points. The sentiment dataset is the dataset with the most significant performance changes, even though it has the fewest."
      }
    ],
    [
      {
        "page_index": 6,
        "page_content": "\n<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Model</td><td style='text-align: center;'>Original</td><td style='text-align: center;'>Original (subset)</td><td style='text-align: center;'>DISSIM</td><td style='text-align: center;'>MILES</td><td style='text-align: center;'>Cochrane</td><td style='text-align: center;'>Medeasi</td><td style='text-align: center;'>Simplify Text</td><td style='text-align: center;'>MUSS</td><td style='text-align: center;'>GPT4o mini</td></tr><tr><td colspan=\"10\">AG News - Classification (accuracy)</td></tr><tr><td style='text-align: center;'>DeBERTa Orig</td><td style='text-align: center;'>94.5</td><td style='text-align: center;'>94.34†</td><td style='text-align: center;'>-6.58*†</td><td style='text-align: center;'>-1.07*</td><td style='text-align: center;'>-2.79*</td><td style='text-align: center;'>-3.71*</td><td style='text-align: center;'>-1.58</td><td style='text-align: center;'>-3.16*†</td><td style='text-align: center;'>-0.92†</td></tr><tr><td style='text-align: center;'>DeBERTa Sim.</td><td style='text-align: center;'>90.26</td><td style='text-align: center;'>90.26†</td><td style='text-align: center;'>-3.0*†</td><td style='text-align: center;'>-0.61*</td><td style='text-align: center;'>-0.83*</td><td style='text-align: center;'>-1.7*</td><td style='text-align: center;'>-1.05</td><td style='text-align: center;'>-1.32†</td><td style='text-align: center;'>+0.39†</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>82.96</td><td style='text-align: center;'>80.39†</td><td style='text-align: center;'>1.72†</td><td style='text-align: center;'>0.03</td><td style='text-align: center;'>-2.74*</td><td style='text-align: center;'>-1.26*</td><td style='text-align: center;'>-1.39*</td><td style='text-align: center;'>0.53†</td><td style='text-align: center;'>-0.52†</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>80.12</td><td style='text-align: center;'>78.68†</td><td style='text-align: center;'>-1.44†</td><td style='text-align: center;'>-1.3*</td><td style='text-align: center;'>-1.96*</td><td style='text-align: center;'>-1.48*</td><td style='text-align: center;'>-1.58*</td><td style='text-align: center;'>0.27†</td><td style='text-align: center;'>-5.26*†</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>79.97</td><td style='text-align: center;'>80.26†</td><td style='text-align: center;'>0.92†</td><td style='text-align: center;'>-0.55*</td><td style='text-align: center;'>-0.21</td><td style='text-align: center;'>0.08</td><td style='text-align: center;'>-0.36</td><td style='text-align: center;'>-0.79†</td><td style='text-align: center;'>1.45†</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>-</td><td style='text-align: center;'>84.08†</td><td style='text-align: center;'>-0.4†</td><td style='text-align: center;'>-0.66†</td><td style='text-align: center;'>1.18†</td><td style='text-align: center;'>-0.79†</td><td style='text-align: center;'>± 0.0†</td><td style='text-align: center;'>± 0.0†</td><td style='text-align: center;'>-0.53†</td></tr><tr><td colspan=\"10\">Sentiment - Classification (accuracy)</td></tr><tr><td style='text-align: center;'>DeBERTa Orig</td><td style='text-align: center;'>88.16</td><td style='text-align: center;'>86.08†</td><td style='text-align: center;'>-6.0*†</td><td style='text-align: center;'>-13.91*</td><td style='text-align: center;'>-1.98*</td><td style='text-align: center;'>-5.65*</td><td style='text-align: center;'>-0.82</td><td style='text-align: center;'>+0.41†</td><td style='text-align: center;'>-0.21†</td></tr><tr><td style='text-align: center;'>DeBERTa Sim.</td><td style='text-align: center;'>87.49</td><td style='text-align: center;'>87.53†</td><td style='text-align: center;'>-6.46*</td><td style='text-align: center;'>-12.57*†</td><td style='text-align: center;'>-1.73*</td><td style='text-align: center;'>-3.8*</td><td style='text-align: center;'>-1.13</td><td style='text-align: center;'>-1.24†</td><td style='text-align: center;'>-3.4*†</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>67.78</td><td style='text-align: center;'>67.84†</td><td style='text-align: center;'>0.2†</td><td style='text-align: center;'>-4.9*</td><td style='text-align: center;'>-16.71*</td><td style='text-align: center;'>0.64</td><td style='text-align: center;'>-5.85*</td><td style='text-align: center;'>-1.45†</td><td style='text-align: center;'>-3.2†</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>68.17</td><td style='text-align: center;'>68.56†</td><td style='text-align: center;'>8.04*†</td><td style='text-align: center;'>-8.95*</td><td style='text-align: center;'>-20.57*</td><td style='text-align: center;'>-1.1</td><td style='text-align: center;'>-14.39*</td><td style='text-align: center;'>-7.01*†</td><td style='text-align: center;'>-6.5*†</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>78.23</td><td style='text-align: center;'>78.76†</td><td style='text-align: center;'>-7.11*†</td><td style='text-align: center;'>-3.96*</td><td style='text-align: center;'>-10.1*</td><td style='text-align: center;'>-1.98*</td><td style='text-align: center;'>-5.97*</td><td style='text-align: center;'>-4.74*†</td><td style='text-align: center;'>-1.96†</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>80.84</td><td style='text-align: center;'>80.72†</td><td style='text-align: center;'>-2.88†</td><td style='text-align: center;'>-4.09*</td><td style='text-align: center;'>-14.76*</td><td style='text-align: center;'>-1.01*</td><td style='text-align: center;'>-9.8*</td><td style='text-align: center;'>-3.19†</td><td style='text-align: center;'>-0.72†</td></tr><tr><td colspan=\"10\">TL;DR - Classification (accuracy)</td></tr><tr><td style='text-align: center;'>DeBERTa Orig</td><td style='text-align: center;'>76.32</td><td style='text-align: center;'>-</td><td style='text-align: center;'>-4.91*</td><td style='text-align: center;'>-1.39</td><td style='text-align: center;'>-15.37*</td><td style='text-align: center;'>-0.25</td><td style='text-align: center;'>-2.27*</td><td style='text-align: center;'>-1.01</td><td style='text-align: center;'>-1.26</td></tr><tr><td style='text-align: center;'>DeBERTa Sim.</td><td style='text-align: center;'>74.56</td><td style='text-align: center;'>-</td><td style='text-align: center;'>-3.53*</td><td style='text-align: center;'>-0.13</td><td style='text-align: center;'>-9.07*</td><td style='text-align: center;'>+0.25</td><td style='text-align: center;'>-0.38</td><td style='text-align: center;'>+0.13</td><td style='text-align: center;'>+0.13</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>62.72</td><td style='text-align: center;'>-</td><td style='text-align: center;'>-3.27*</td><td style='text-align: center;'>-3.9*</td><td style='text-align: center;'>-5.29*</td><td style='text-align: center;'>-4.66*</td><td style='text-align: center;'>-3.78*</td><td style='text-align: center;'>-3.02*</td><td style='text-align: center;'>-3.9*</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>44.84</td><td style='text-align: center;'>-</td><td style='text-align: center;'>5.41*</td><td style='text-align: center;'>-3.4*</td><td style='text-align: center;'>-1.26</td><td style='text-align: center;'>-3.15</td><td style='text-align: center;'>0.75</td><td style='text-align: center;'>± 0.0</td><td style='text-align: center;'>-3.91*</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>56.55</td><td style='text-align: center;'>-</td><td style='text-align: center;'>-5.54*</td><td style='text-align: center;'>-5.79*</td><td style='text-align: center;'>-4.91*</td><td style='text-align: center;'>-6.68*</td><td style='text-align: center;'>-2.27</td><td style='text-align: center;'>-1.01</td><td style='text-align: center;'>-1.13</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>65.74</td><td style='text-align: center;'>-</td><td style='text-align: center;'>-0.88</td><td style='text-align: center;'>± 0.0</td><td style='text-align: center;'>± 0.0</td><td style='text-align: center;'>-2.39</td><td style='text-align: center;'>-2.01</td><td style='text-align: center;'>-0.75</td><td style='text-align: center;'>-0.75</td></tr><tr><td colspan=\"10\">TL;DR - Summarization (rougeL)</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>23.09</td><td style='text-align: center;'>-</td><td style='text-align: center;'>1.1*</td><td style='text-align: center;'>-2.04*</td><td style='text-align: center;'>-5.95*</td><td style='text-align: center;'>-4.59*</td><td style='text-align: center;'>-2.17*</td><td style='text-align: center;'>-0.88*</td><td style='text-align: center;'>-0.79*</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>23.89</td><td style='text-align: center;'>-</td><td style='text-align: center;'>0.44</td><td style='text-align: center;'>-3.17*</td><td style='text-align: center;'>-6.4*</td><td style='text-align: center;'>-6.08*</td><td style='text-align: center;'>-2.34*</td><td style='text-align: center;'>-1.37*</td><td style='text-align: center;'>-0.98*</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>27.04</td><td style='text-align: center;'>-</td><td style='text-align: center;'>1.44*</td><td style='text-align: center;'>-2.81*</td><td style='text-align: center;'>-7.43*</td><td style='text-align: center;'>-7.04*</td><td style='text-align: center;'>-2.9*</td><td style='text-align: center;'>-1.62*</td><td style='text-align: center;'>-0.76</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>24.57</td><td style='text-align: center;'>-</td><td style='text-align: center;'>0.56</td><td style='text-align: center;'>-2.56*</td><td style='text-align: center;'>-6.42*</td><td style='text-align: center;'>-5.64*</td><td style='text-align: center;'>-2.27*</td><td style='text-align: center;'>-1.09*</td><td style='text-align: center;'>-0.61*</td></tr></table>\n\n<div style=\"text-align: center;\">Table 4: Changes in performance across all English datasets. For most of the models and simplifiers, the scores decrease (red boxes). Only a few combinations show improved performance (blue boxes). * statistically significant change  $ (p < 0.05) $ , significant changes have a darker color,  $ \\dagger $  evaluated and compared only on the fixed subset</div>\n\n\nand most distinct classes. The performance decreases are especially remarkable for the DeBERTa classifier, which was fine-tuned on simplified data. This model exhibits a drop in performance even when the same simplifier is used for training and testing. A similar problem can be observed with GPT4o-mini, which exhibits a performance drop even when it is working on its own simplification outputs. However, statistically significant performance changes on the GPT4o-mini simplifications are scarce.\n\n### 4.3 Human evaluation\n\nOur results show that all classifiers, even powerful LLMs like GPT-4o-mini, exhibit a performance decrease when working with simplified inputs. An obvious explanation for this behavior would be that the simplification systems alter the meaning of the input samples. To examine the meaning preservation of the simplifications, we conducted a human evaluation on all simplifiers except DISSIM. DISSIM is a rule-based, syntactic-only system, so it cannot alter the meaning. We randomly selected 12 samples from each of the three datasets and showed the original and simplified versions to a simple language expert (one of the authors). The samples were presented one by one, and we randomized the order of the simplifiers so that the annotator did not know which models created the simplification. Overall, we analyzed 216 original-simplified pairs (12 samples across 3 datasets and 6 simplifiers). The annotator graded the samples on three different aspects: content preservation, the existence of a hallucination, and whether the simplified sample preserved the original label. The content and label preservation were ranked on a 4-point Likert scale, while the hallucinations received a binary label.\n\n\n\nThe most relevant finding is that only nine out of 216 samples changed the original label, i.e., 96% of the analyzed samples preserved the labels and, thus, should receive the same prediction by the classifiers. In contrast, the results from the content and hallucination evaluation paint a less clear picture, as can be seen in Figure 3. While Medeasi, MUSS, and GPT4o-mini preserve most of the content with almost no hallucinations, the Cochrane and Simpli"
      }
    ],
    [
      {
        "label": "fig",
        "figure_path": "imgs/img_in_chart_box_142_141_647_433.jpg"
      },
      {
        "label": "fig",
        "figure_path": "imgs/img_in_chart_box_660_140_1046_454.jpg"
      },
      {
        "page_index": 7,
        "page_content": "<div style=\"text-align: center;\"><img src=\"imgs/img_in_chart_box_142_141_647_433.jpg\" alt=\"Image\" width=\"42%\" /></div>\n\n\n<div style=\"text-align: center;\">(a) Content preservation of the simplified versions across the three English datasets</div>\n\n\n<div style=\"text-align: center;\"><img src=\"imgs/img_in_chart_box_660_140_1046_454.jpg\" alt=\"Image\" width=\"32%\" /></div>\n\n\n<div style=\"text-align: center;\">(b) Number of hallucinations per simplifier</div>\n\n\n<div style=\"text-align: center;\">Figure 3: Results from human evaluation. GPT4o-mini, Medeasi, and MUSS show the best content preservation and the least hallucinations.</div>\n\n\n\n<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Model</td><td style='text-align: center;'>Orig.</td><td style='text-align: center;'>DEplain</td><td style='text-align: center;'>MILES</td><td style='text-align: center;'>GPT4o mini</td></tr><tr><td colspan=\"5\">Gnad10 - Classification (accuracy)</td></tr><tr><td style='text-align: center;'>FRE</td><td style='text-align: center;'>46.41</td><td style='text-align: center;'>61.34</td><td style='text-align: center;'>59.96</td><td style='text-align: center;'>52.55</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>26.75</td><td style='text-align: center;'>+7.1*</td><td style='text-align: center;'>+2.34*</td><td style='text-align: center;'>+4.28*</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>50.78</td><td style='text-align: center;'>-5.64*</td><td style='text-align: center;'>-3.7*</td><td style='text-align: center;'>+0.19</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>33.85</td><td style='text-align: center;'>+7.4*</td><td style='text-align: center;'>-1.85</td><td style='text-align: center;'>+7.88*</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>58.95</td><td style='text-align: center;'>-4.77*</td><td style='text-align: center;'>+3.21*</td><td style='text-align: center;'>+1.17</td></tr><tr><td colspan=\"5\">ML SUM DE - Classification (accuracy)</td></tr><tr><td style='text-align: center;'>FRE</td><td style='text-align: center;'>48.84</td><td style='text-align: center;'>61.06</td><td style='text-align: center;'>62.32</td><td style='text-align: center;'>53.25</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>49.74</td><td style='text-align: center;'>+3.46</td><td style='text-align: center;'>-1.73</td><td style='text-align: center;'>+3.11</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>62.0</td><td style='text-align: center;'>-1.9</td><td style='text-align: center;'>-0.51</td><td style='text-align: center;'>+2.42</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>61.14</td><td style='text-align: center;'>$ \\pm $  0.0</td><td style='text-align: center;'>-6.74*</td><td style='text-align: center;'>+5.18*</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>77.72</td><td style='text-align: center;'>-7.77*</td><td style='text-align: center;'>-2.07*</td><td style='text-align: center;'>-1.55</td></tr><tr><td colspan=\"5\">ML SUM DE - Summarization (rougeL)</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>17.46</td><td style='text-align: center;'>-10.97*</td><td style='text-align: center;'>-3.05*</td><td style='text-align: center;'>-1.7*</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>14.78</td><td style='text-align: center;'>-9.19*</td><td style='text-align: center;'>-1.99*</td><td style='text-align: center;'>-0.71</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>15.63</td><td style='text-align: center;'>-9.08*</td><td style='text-align: center;'>-1.43*</td><td style='text-align: center;'>+0.65</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>16.1</td><td style='text-align: center;'>-9.98*</td><td style='text-align: center;'>-1.4*</td><td style='text-align: center;'>+0.24</td></tr></table>\n\n<div style=\"text-align: center;\">Table 5: Accuracy changes on German data, * statistically significant change (p < 0.05)</div>\n\n\nfyText simplifiers show some content alterations. MILES is a lexical simplification system that performs minimal changes and shows decent content preservation. Nevertheless, it is among the simplifiers with the strongest performance drops for the classifiers. This indicates that the choice of words in simplified language is more relevant to the classifiers than the sheer number of edit operations. This aligns with previous research by Anschütz et al. (2024), who find that the Levenshtein distance between original and simplified samples only has a weak correlation with label changes in LLMs.\n\nOverall, human evaluation could verify our assumption from subsection 3.1: While the simplifiers might change small aspects, these changes do\n\n<div style=\"text-align: center;\">Table 6: Accuracy changes on Russian data, * statistically significant change (p < 0.05)</div>\n\n\nnot affect the selected classification tasks, and the overall labels are preserved (some examples are presented in Appendix A). Therefore, we reject faithfulness alone as a trivial explanation for the LLM’s bad generalization performance.\n\n### 4.4 Non-English data\n\nTable 5 and Table 6 show the results for German and Russian respectively. First of all, we can see that the FRE scores increase for all ATS systems, indicating that the simplifiers successfully improved the readability of the samples. Again, the GPT4o-mini simplifications achieve a comparatively small readability improvement. For Russian, we observe hardly any statistically significant changes, except for some strong improvements of Aya Expanse on the classification task. In general, both Russian models show an extremely weak summarization performance in terms of rougeL score, even for the original data. Therefore, the changes on simplified data are only of minor importance as the models don't seem to fulfill the task at all. For German, we observe many improvements, especially for the"
      }
    ],
    [
      {
        "page_index": 8,
        "page_content": "Gnad10 classification task. In addition, simplifications by GPT4o show the most significant improvements and only one significant performance drop. This is even the case in the summarization task. Our results allow for two interpretations: Most models are primarily trained on English, and they seem to overfit more to the standard language style in their pre-training there $ ^{2} $ . Therefore, their performance on English simplified language drops significantly. Second, for languages with weaker LLM support, we expect less overfitting. Thus, these models can benefit from simplifications, especially if they are of high, human-like quality, as with GPT4o-mini.\n\n## 5 Conclusion\n\nExperiments across six datasets, nine ATS systems, and three languages show that English LLMs exhibit a severe performance drop when switching from original to simplified language, uncovering a weak generalization to this language style. However, simplified texts can enhance performance at inference time for non-English languages. We thus encourage content creators to prioritize using simple language online as a way to improve LLMs' downstream performance and comprehension and to open their models to a broader audience.\n\n## Limitations\n\nWe provide an extensive evaluation of the employed simplification models, evaluating them for their simplicity gain, simplification quality, and meaning preservation with automatic metrics. In addition, we conducted a human evaluation to verify our label preservation assumption. However, due to the large scope of our experiments with multiple datasets and simplifiers, we could only evaluate 12 samples per dataset and simplifier combination. The results of this evaluation paint a clear picture, with more than 95% of the samples preserving the original label. Nevertheless, this evaluation could be extended to more samples, evaluation aspects, and non-English languages.\n\nIn addition to this, our investigation only covers a limited set of NLP tasks. We selected the sentiment and classification tasks to avoid biases due to automatic evaluation metrics and insufficient meaning preservation of the simplification models. As shown in our human evaluation, this task selection was valuable as the simplifications sometimes altered the content but preserved the original label. In addition, we tested the performance on summarization as a generation task. Nevertheless, it would be interesting to add further NLP tasks to draw a broader picture of LLM generalization on simplified language. Moreover, since the results indicate that simplifications can improve the performance of non-English languages, this research should be extended to further languages.\n\n\n\nFinally, we used the same prompts for all models and tested them in a zero-shot setting. This could mean that the models could not unfold their full potential and that the performances could be improved further. However, we don’t evaluate the models on an absolute scale; rather, we compare the performance of simplified and original texts. All experiments are conducted under the same setting, and thus, the limitations of the zero-shot setting should not affect our overall results. Another problem could be data contamination. Since our datasets are quite old, it is likely that they were included in the LLM pre-training data. However, our paper measures the generalization of the LLMs on simplified language. Thus, this change in behavior on unseen data is actually part of our investigation, and the potential data contamination does not affect the validity of our findings.\n\n## Ethical considerations\n\nThe main goal of text simplification is to increase the accessibility of information to everybody. Yet, simplified language can also be perceived as discrimination and may introduce bias to the users (Maaß, 2020). While we assume that the availability and the option to choose between different language levels are a benefit, automatic simplifications can remove critical information, and thus, should not be deployed without further human control. Nevertheless, for many people, the usage of simplified language is indispensable for their participation and autonomy, while it does not disturb the user experience for stronger readers (Stodden and Nguyen, 2024). Therefore, LLMs should offer support for this style of language, no matter the possible discrimination. However, we find some alarming behavior in most of the LLMs, as our results show that they decrease their performance when using simplified language in English. This can have severe implications for people with low"
      }
    ],
    [
      {
        "page_index": 9,
        "page_content": "literacy or mental disabilities when using platforms like ChatGPT: When a user asks the chatbot for a summarization of a news snippet in plain language, the models are more likely to make mistakes in these interactions. These people are already a vulnerable target group that struggles to verify information on the internet due to information barriers of overly complicated texts. When easy-to-use and trust-evoking platforms like chatbots show a worse performance when interacting with those people, this implies severe discrimination against users of simplified language that we uncovered with this work.\n\n## References\n\nSweta Agrawal and Marine Carpuat. 2024. Do text simplification systems preserve meaning? a human evaluation via reading comprehension. Transactions of the Association for Computational Linguistics, 12:432–448.\n\nToni Amstad. 1978. Wie verständlich sind unsere Zeitungen? Ph.D. thesis, Universität Zürich.\n\nMiriam Anschütz, Edoardo Mosca, and Georg Groh. 2024. Simpler becomes harder: Do LLMs exhibit a coherent behavior on simplified corpora? In Proceedings of the Workshop on DeTermIt! Evaluating Text Difficulty in a Multilingual Context @ LREC-COLING 2024, pages 185–195, Torino, Italia. ELRA and ICCL.\n\nChandrayee Basu, Rosni Vasu, Michihiro Yasunaga, and Qian Yang. 2023. Med-easi: Finely annotated dataset and models for controllable simplification of medical texts. Preprint, arXiv:2302.09155.\n\nDavid Beauchemin, Horacio Saggion, and Richard Khoury. 2023. Meaningbert: assessing meaning preservation between sentences. Frontiers in Artificial Intelligence, 6.\n\nJohn Dang, Shivalika Singh, Daniel D'souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker. 2024. Aya expanse: Combining research breakthroughs for a new multilingual frontier. Preprint, arXiv:2412.04261.\n\nAshwin Devaraj, Iain Marshall, Byron Wallace, and Junyi Jessy Li. 2021. Paragraph-level simplification of medical texts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4972–4984, Online. Association for Computational Linguistics.\n\nAnna Dmitrieva and Jörg Tiedemann. 2021. Creating an aligned Russian text simplification dataset from language learner data. In Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing, pages 73–79, Kiyv, Ukraine. Association for Computational Linguistics.\n\nRudolph Flesch. 1948. A new readability yardstick. Journal of applied psychology, 32(3):221.\n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU – neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46–68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, and Ava Spataru et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783.\n\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. Preprint, arXiv:2111.09543.\n\nDavid Heineman, Yao Dou, Mounica Maddela, and Wei Xu. 2023. Dancing between success and failure: Edit-level simplification evaluation using SALSA. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3466–3495, Singapore. Association for Computational Linguistics.\n\nYichen Huang and Ekaterina Kochmar. 2024. REF\\-eREE: A Reference\\-FREE model\\-based metric for text simplification. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC\\-COLING 2024), pages 13740–13753, Torino, Italia. ELRA and ICCL.\n\nLudivine Javourey-Drevet, Stéphane Dufau, Thomas François, Núria Gala, Jacques Ginestié, and Johannes C. Ziegler. 2022. Simplification of literary and scientific texts to improve reading fluency and comprehension in beginning readers of french. Applied Psycholinguistics, 43(2):485–512."
      }
    ],
    [
      {
        "page_index": 10,
        "page_content": "Tannon Kew, Alison Chi, Laura Vásquez-Rodríguez, Sweta Agrawal, Dennis Aumiller, Fernando Alva-Manchego, and Matthew Shardlow. 2023. BLESS: Benchmarking large language models on sentence simplification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13291–13309, Singapore. Association for Computational Linguistics.\n\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332–9346, Online. Association for Computational Linguistics.\n\nPhilippe Laban, Tobias Schnabel, Paul Bennett, and Marti A. Hearst. 2021. Keep it simple: Unsupervised simplification of multi-paragraph text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6365–6378, Online. Association for Computational Linguistics.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.\n\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.\n\nChristiane Maaß. 2020. Easy language–plain language–easy language plus: Balancing comprehensibility and acceptability. Frank & Timme, Berlin.\n\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Research article. Journal of the Association for Information Science and Technology, 65(4):782 – 796.\n\nLouis Martin, Angela Fan, Éric de la Clergerie, Antoine Bordes, and Benoît Sagot. 2022. MUSS: Multilingual unsupervised sentence simplification by mining paraphrases. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1651–1664, Marseille, France. European Language Resources Association.\n\nSneha Mehta, Bahareh Azarnoush, Boris Chen, Avneesh Saluja, Vinith Misra, Ballav Bihani, and Ritwik Kumar. 2020. Simplify-then-translate: Automatic preprocessing for black-box translation. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8488–8495.\n\nRei Miyata and Midori Tatsumi. 2019. Evaluating the suitability of human-oriented text simplification for machine translation. In Proceedings of the 33rd Pacific Asia Conference on Language, Information and Computation, pages 147–155. Waseda University.\n\nDennis Murphy Odo. 2022. The Effect of Automatic Text Simplification on L2 Readers' Text Comprehension. Applied Linguistics, 44(6):1030–1046.\n\nChristina Niklaus, Matthias Cetto, André Freitas, and Siegfried Handschuh. 2019. DisSim: A discourse-aware syntactic text simplification framework for English and German. In Proceedings of the 12th International Conference on Natural Language Generation, pages 504–507, Tokyo, Japan. Association for Computational Linguistics.\n\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812–4829, Online. Association for Computational Linguistics.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\nJipeng Qiang, Yun Li, Yi Zhu, Yunhao Yuan, and Xindong Wu. 2020. Lsbert: A simple framework for lexical simplification. Preprint, arXiv:2006.14939.\n\nMichael Ryan, Tarek Naous, and Wei Xu. 2023. Revisiting non-English text simplification: A unified multilingual benchmark. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4898–4927, Toronto, Canada. Association for Computational Linguistics.\n\nAndrey Sakhovskiy, Alexandra Izhevskaya, Alena Pestova, Elena Tutubalina, Valentin Malykh, Ivan Smurov, and Ekaterina Artemova. 2021. Rusimplesenteval-2021 shared task: evaluating sentence simplification for Russian. In Proceedings of the International Conference “Dialogue, pages 607–617.\n\nAndreas Säuberli, Franz Holzknecht, Patrick Haller, Silvana Deilen, Laura Schiffl, Silvia Hansen-Schirra, and Sarah Ebling. 2024. Digital comprehensibility assessment of simplified texts among persons with intellectual disabilities. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI '24, New York, NY, USA. Association for Computing Machinery.\n\nDietmar Schabus, Marcin Skowron, and Martin Trapp. 2017. One million posts: A data set of german online discussions. In Proceedings of the 40th International ACM SIGIR Conference on Research and"
      }
    ],
    [
      {
        "page_index": 11,
        "page_content": "Development in Information Retrieval (SIGIR), pages 1241–1244, Tokyo, Japan.\n\nJordan Schmidek and Denilson Barbosa. 2014. Improving open relation extraction via sentence restructuring. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 3720–3723, Reykjavik, Iceland. European Language Resources Association (ELRA).\n\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2020. MLSUM: The multilingual summarization corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8051–8067, Online. Association for Computational Linguistics.\n\nSanja Štajner and Maja Popovic. 2016. Can text simplification help machine translation? In Proceedings of the 19th Annual Conference of the European Association for Machine Translation, pages 230–242.\n\nRegina Stodden. 2024. Reproduction of German text simplification systems. In Proceedings of the Workshop on DeTermIt! Evaluating Text Difficulty in a Multilingual Context @ LREC-COLING 2024, pages 1–15, Torino, Italia. ELRA and ICCL.\n\nRegina Stodden, Omar Momen, and Laura Kallmeyer. 2023. DEplain: A German parallel corpus with intralingual translations into plain language for sentence and document simplification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16441–16463, Toronto, Canada. Association for Computational Linguistics.\n\nRegina Stodden and Phillip Nguyen. 2024. Can text simplification help to increase the acceptance of E-participation? In Proceedings of the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024, pages 20–32, Torino, Italia. ELRA and ICCL.\n\nJan Trienes, Sebastian Joseph, Jörg Schlötterer, Christin Seifert, Kyle Lo, Wei Xu, Byron C. Wallace, and Junyi Jessy Li. 2024. InfoLossQA: Characterizing and recovering information loss in text simplification. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 4263–4294.\n\nHoang Van, Zheng Tang, and Mihai Surdeanu. 2021. How may I help you? using neural text simplification to improve downstream NLP tasks. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4074–4080, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nDavid Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In Proceedings of ACL-08: HLT, pages 344–352, Columbus, Ohio. Association for Computational Linguistics.\n\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.\n\n## A Examples form human evaluation\n\nSee Table 7 for examples where the content is altered by the simplifier but the overall label is still preserved.\n\n## B LLM simplification prompts\n\nWe used GPT4o-mini to create high-quality simplifications. We used the following prompt where sample is replaced by the text to be predicted. For German and Russian, the prompt is translated, respectively.\n\nSimplify (EN): {“role”: {“system”, “content”: “You are a helpful assistant. You will be provided with sentences from news articles. Your task is to simplify the texts to enhance readability. You must not alter the meaning and don’t provide reasoning.”},\n\n{\"role\": \"user\", \"content\": \"{{sample} - Simplification: }\"}\n\nSimplify DE: {“role”: {“system”, “content”: “Du bist ein hilfreicher Assistent. Du bekommst Sätze aus Nachrichtenartikeln. Deine Aufgabe ist es, die Texte zu vereinfachen, um die Verständlichkeit zu erhöhen. Du darfst den Inhalt nicht verändern und brauchst keine Begründungen angeben.”},\n\nSimplify RU: {“role”: {“system”, “content”: “Ты - полезный помощник. Тебе будут предоставлены предложения из новостных статей. Твоя задача - упростить текст, чтобы повысить его читабельность. Ты не должен изменять смысл и приводить аргументы.”}, {“role”: “user”, “content”: “{sample} - Упрощение: ”}\n\n## C LLM Prediction prompts\n\nWe used the same system prompts for all four large language models and prompted them in a zero-shot manner. The prompts differ per dataset and language. Below are the prompts we used for the classification and summarization tasks where sample is replaced by the text to be predicted."
      }
    ],
    [
      {
        "page_index": 12,
        "page_content": "\n<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Original</td><td style='text-align: center;'>Simplified</td><td style='text-align: center;'>Label</td></tr><tr><td style='text-align: center;'>Sudan Peace Talks Resume for South as Tensions Brew KHARTOUM/NAIROBI (Reuters) - Sudan’s government resumed talks with rebels in the oil-producing south on Thursday while the United Nations set up a panel to investigate charges of genocide in the west of Africa’s largest country.</td><td style='text-align: center;'>Sudan peace talks resume in south as tensions rise KHARTOUM/NAIROBI (Reuters) - Sudan’s government held peace talks on Thursday with south-west rebels, while the United Nations set up a panel to investigate allegations of genocide in the world’s largest country.</td><td style='text-align: center;'>world</td></tr><tr><td style='text-align: center;'>Operating income rose to EUR 696.4 mn from EUR 600.3 mn in 2009.</td><td style='text-align: center;'>This year’s net profit more than doubled to EUR 696.4 mn from EUR 600.3 mn in 2009.</td><td style='text-align: center;'>positive</td></tr><tr><td style='text-align: center;'>All art establishments are concerned with the degradation of paintings. Harmful factors such as sunlight, moisture, and certain volatile organic compounds can accelerate degradation. Graphene may be the solution to protecting art from exposure to harmful agents. A one-atom-thick sheet of graphene can adhere easily to various substrates and serve as an excellent barrier against oxygen, gases, moisture, and UV light. The graphene sheets can be added to framing glass for artworks with extremely rough surfaces or embossed patterns. The sheets can be removed using a soft rubber eraser.</td><td style='text-align: center;'>All art establishments are concerned with the degradation of paintings. Harmful factors such as sunlight, moisture, and certain volatile organic compounds can accelerate the process of deterioration. Graphene, which is made of a variety of materials, can be applied to framing glass to protect against oxygen, gases, and UV light. It can also be used as a barrier against bacteria and fungi, which can cause skin irritation.</td><td style='text-align: center;'>Science &amp; Futuristic Technology</td></tr></table>\n\n<div style=\"text-align: center;\">Table 7: Examples from the human evaluation. All simplifications are factually incorrect or introduce hallucinations (bolded parts). Even with these content errors, the original labels are preserved.</div>\n\n\nAG News (EN): {“role”: {“system”, “content”: “You are a helpful assistant. You will be provided with sentences from news articles. Classify each query into a news topic. There are four possible topics: world, sports, business or sci/tech. You must not choose another topic. Answer only with one single word and do not provide reasoning.”}, {“role”: “user”, “content”: “{sample} - The topic is”}\n\nSentiment (EN): {“role”: {“system”, “content”: “You are a helpful assistant. You will be provided with sentences from articles. Classify the sentiment of each query. There are three possible sentiments: positive, neutral or negative. You must not choose another sentiment. Answer only with one single word and do not provide reasoning.”},\n\n{\"role\": \"user\", \"content\": \"{{sample} - The sentiment is\"}\n\nTL;DR (EN): {\"role\": {\"system\", \"content\": \"You are a helpful assistant. You will be provided with sentences from news articles. Classify each query into a news topic. There are five possible topics: 'Sponsor', 'Big Tech & Startups', 'Science & Futuristic Technology', 'Programming & Design & Data Science' and 'Miscellaneous'. You must not choose another topic. Answer only with one single word and do not provide reasoning.\"}, {\"role\": \"user\", \"content\": \"{{sample} - The topic is\"}\n\n\n\nGnad10 (DE): {“role”: {“system”, “content”: “Du bist ein hilfreicher Assistent. Du bekommst Sätze aus Nachrichtentartikeln. Ordne jede Anfrage einem Nachrichtenthema zu. Es gibt neun mögliche Themen: Web, Panorama, International, Wirtschaft, Sport, Inland, Etat, Wissenschaft und Kultur. Du darfst kein anderes Thema wählen. Antworte nur mit einem einzigen Wort und gib"
      }
    ],
    [
      {
        "page_index": 13,
        "page_content": "keine Begründung an.\"},\n\n“role”: “user”, “content”: “{sample} - Das Thema ist”\n\nML SUM (DE): {“role”: {“system”, “content”: “Du bist ein hilfreicher Assistent. Du bekommst Sätze aus Nachrichtenartikeln. Ordne jede Anfrage einem Nachrichtenthema zu. Es gibt zwölf mögliche Themen: politik, wirtschaft, geld, panorama, sport, muenchen, digital, karriere, bildung, reise, auto und stil. Du darfst kein anderes Thema wählen. Antworte nur mit einem einzigen Wort und gib keine Begründung an.”},\n\n{\"role\": \"user\", \"content\": \"{{sample} - Das Thema ist\"}\n\nML SUM (RU): {“role”: {“system”, “content”: “Ты - полезный ассистент. Тебе будут предоставлены предложения из новостных статей. Классифицируй каждый запрос в соответствии с темой новости. Темы даны на английском языке, и есть девять возможных тем: science, politics, mosobl, culture, social, incident, economics, sport, moscow. Ты не должен выбирать какую-либо другую тему. Отвечай только одним словом и не объясняй.”},\n\n{\"role\":\"user\",\"content\":\"{{sample} - Tema}}\n\nSummarize (EN): {“role”: {“system”, “content”: “You are a helpful assistant. You will be provided with sentences from news articles. Your task is to create a headline that summarizes the content. Answer only with one sentence and don’t provide reasoning.”},\n\n{\"role\": \"user\", \"content\": \"{{sample} - The headline is\"}\n\nSummarize DE: {“role”: {“system”, “content”: “Du bist ein hilfreicher Assistent. Du bekommst Sätze aus Nachrichtenartikeln. Deine Aufgabe ist es, einen Titel zu verfassen, der den Inhalt zusammenfasst. Antworte nur mit einem Satz und gib keine Begründung an.”},\n\n{\"role\": \"user\", \"content\": \"{{sample} - Der Titel ist\"}\n\nSummarize RU: {“role”: {“system”, “content”: “Ты - полезный помощник. Тебе будут предоставлены предложения из новостных статей. Твоя задача - придумать заголовок, который обобщает содержание статьи. Отвечай только одним предложением и не приводи аргументы.”}, {“role”: “user”, “content”: “{sample} - Заголовок:”}\n\n\n\n## D Further summarization metrics\n\nPrevious work has shown that overlap-based metrics like rougeL are insufficient to cover all aspects of language generation tasks (Freitag et al., 2022). For this, we evaluated the headline generation task with a collection of different metrics. The results are presented in Table 8.\n\nUnfortunately, BERTscore does not seem to detect any changes in the headlines. However, this is not due to the headlines being equally good, but rather a matter of BERTscore that overvalues single concepts and words. This becomes evident in the following example from the TL;DR dataset (simplified using GPT4o-mini, predicted headlines by AyaExpanse8B):\n\nReference headline: \"Instagram’s Co-Founders Said to Step Down From Company\"\n\nPredicted headline (based on orig text): \"Instagram Co-Founders Kevin Systrom and Mike Krieger Resign from Facebook\"\n\n→ BERTscore: 0.8669\n\nPredicted head (based on simple text): \"Instagram Co-Founders Kevin Systrom and Mike Krieger Resign, Raising Questions About Facebook's Future\"\n\n→ BERTscore 0.8660\n\nThe simplified headline hallucinates \"Raising Questions About Facebook's Future\", but this hallucination is not reflected in the scores.\n\nTo overcome this issue, we also employed an LLM judge with gemma-3-27b-it. We prompted it to evaluate how well the candidate headline fits the reference headline on the same scale as in our human evaluation (from 0 (no fit) to 3 (good fit)). The results are presented in the last block of Table 8. Here, the shortcomings of the headlines generated from the simplified texts are more evident.\n\nFinally, an even better evaluation approach would be to use the LLM judge to perform unsupervised evaluation, i.e., compare the headlines with the input texts directly. However, since we found that LLMs have a non-trustworthy behavior on simplified inputs, we fear that an LLM judge would also output wrong scores. Therefore, we kept the setup of only comparing the generated headline to the reference."
      }
    ],
    [
      {
        "page_index": 14,
        "page_content": "\n<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Model</td><td style='text-align: center;'>Original</td><td style='text-align: center;'>DISSIM</td><td style='text-align: center;'>MILES</td><td style='text-align: center;'>Cochrane</td><td style='text-align: center;'>Medeasi</td><td style='text-align: center;'>Simplify Text</td><td style='text-align: center;'>MUSS</td><td style='text-align: center;'>GPT4o mini</td></tr><tr><td colspan=\"9\">TL;DR - Headline generation (rougeL)</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>23.09</td><td style='text-align: center;'>1.1*</td><td style='text-align: center;'>-2.04*</td><td style='text-align: center;'>-5.95*</td><td style='text-align: center;'>-4.59*</td><td style='text-align: center;'>-2.17*</td><td style='text-align: center;'>-0.88*</td><td style='text-align: center;'>-0.79*</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>23.89</td><td style='text-align: center;'>0.44</td><td style='text-align: center;'>-3.17*</td><td style='text-align: center;'>-6.4*</td><td style='text-align: center;'>-6.08*</td><td style='text-align: center;'>-2.34*</td><td style='text-align: center;'>-1.37*</td><td style='text-align: center;'>-0.98*</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>27.04</td><td style='text-align: center;'>1.44*</td><td style='text-align: center;'>-2.81*</td><td style='text-align: center;'>-7.43*</td><td style='text-align: center;'>-7.04*</td><td style='text-align: center;'>-2.9*</td><td style='text-align: center;'>-1.62*</td><td style='text-align: center;'>-0.76</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>24.57</td><td style='text-align: center;'>0.56</td><td style='text-align: center;'>-2.56*</td><td style='text-align: center;'>-6.42*</td><td style='text-align: center;'>-5.64*</td><td style='text-align: center;'>-2.27*</td><td style='text-align: center;'>-1.09*</td><td style='text-align: center;'>-0.61*</td></tr><tr><td colspan=\"9\">TL;DR - Headline generation (BLEU)</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>3.86</td><td style='text-align: center;'>0.67*</td><td style='text-align: center;'>0.21</td><td style='text-align: center;'>-0.92*</td><td style='text-align: center;'>-0.69*</td><td style='text-align: center;'>-0.48*</td><td style='text-align: center;'>-0.2</td><td style='text-align: center;'>-0.28*</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>4.11</td><td style='text-align: center;'>0.12</td><td style='text-align: center;'>-0.34*</td><td style='text-align: center;'>-0.98*</td><td style='text-align: center;'>-0.82*</td><td style='text-align: center;'>-0.46*</td><td style='text-align: center;'>-0.14</td><td style='text-align: center;'>-0.03</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>4.91</td><td style='text-align: center;'>1.14*</td><td style='text-align: center;'>0.01</td><td style='text-align: center;'>-1.2*</td><td style='text-align: center;'>-1.13*</td><td style='text-align: center;'>-0.48*</td><td style='text-align: center;'>-0.07</td><td style='text-align: center;'>0.03</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>4.61</td><td style='text-align: center;'>0.67*</td><td style='text-align: center;'>-0.24*</td><td style='text-align: center;'>-1.27*</td><td style='text-align: center;'>-0.84*</td><td style='text-align: center;'>-0.45*</td><td style='text-align: center;'>-0.25*</td><td style='text-align: center;'>-0.05</td></tr><tr><td colspan=\"9\">TL;DR - Headline generation (BERTscore)</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>0.86</td><td style='text-align: center;'>$ \\pm $  0.0*</td><td style='text-align: center;'>$ \\pm $  0.0*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>0.87</td><td style='text-align: center;'>$ \\pm $  0.0</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>0.88</td><td style='text-align: center;'>0.01*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>0.87</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0*</td><td style='text-align: center;'>$ \\pm $  -0.0</td></tr><tr><td colspan=\"9\">TL;DR - Headline generation (METEOR)</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>0.21</td><td style='text-align: center;'>0.01*</td><td style='text-align: center;'>-0.03*</td><td style='text-align: center;'>-0.07*</td><td style='text-align: center;'>-0.06*</td><td style='text-align: center;'>-0.03*</td><td style='text-align: center;'>-0.01*</td><td style='text-align: center;'>-0.01*</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>0.22</td><td style='text-align: center;'>$ \\pm $  0.0</td><td style='text-align: center;'>-0.04*</td><td style='text-align: center;'>-0.08*</td><td style='text-align: center;'>-0.07*</td><td style='text-align: center;'>-0.03*</td><td style='text-align: center;'>-0.02*</td><td style='text-align: center;'>-0.01*</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>0.23</td><td style='text-align: center;'>$ \\pm $  -0.0</td><td style='text-align: center;'>-0.03*</td><td style='text-align: center;'>-0.08*</td><td style='text-align: center;'>-0.08*</td><td style='text-align: center;'>-0.04*</td><td style='text-align: center;'>-0.02*</td><td style='text-align: center;'>-0.01*</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>0.23</td><td style='text-align: center;'>-0.06*</td><td style='text-align: center;'>-0.03*</td><td style='text-align: center;'>-0.08*</td><td style='text-align: center;'>-0.07*</td><td style='text-align: center;'>-0.03*</td><td style='text-align: center;'>-0.02*</td><td style='text-align: center;'>-0.01</td></tr><tr><td colspan=\"9\">TL;DR - Headline generation (LLM judge: compare references)</td></tr><tr><td style='text-align: center;'>AyaExpanse8B</td><td style='text-align: center;'>1.46</td><td style='text-align: center;'>0.02</td><td style='text-align: center;'>-0.29*</td><td style='text-align: center;'>-0.46*</td><td style='text-align: center;'>-0.48*</td><td style='text-align: center;'>-0.17*</td><td style='text-align: center;'>-0.1*</td><td style='text-align: center;'>$ \\pm $  -0.0</td></tr><tr><td style='text-align: center;'>Llama3.1 8B</td><td style='text-align: center;'>1.55</td><td style='text-align: center;'>-0.02</td><td style='text-align: center;'>-0.34*</td><td style='text-align: center;'>-0.53*</td><td style='text-align: center;'>-0.56*</td><td style='text-align: center;'>-0.2*</td><td style='text-align: center;'>-0.12*</td><td style='text-align: center;'>-0.06*</td></tr><tr><td style='text-align: center;'>Llama3.1 70B</td><td style='text-align: center;'>1.7</td><td style='text-align: center;'>-0.06</td><td style='text-align: center;'>-0.35*</td><td style='text-align: center;'>-0.58*</td><td style='text-align: center;'>-0.61*</td><td style='text-align: center;'>-0.27*</td><td style='text-align: center;'>-0.15*</td><td style='text-align: center;'>-0.05</td></tr><tr><td style='text-align: center;'>GPT4o-mini</td><td style='text-align: center;'>1.61</td><td style='text-align: center;'>-0.37*</td><td style='text-align: center;'>-0.35*</td><td style='text-align: center;'>-0.52*</td><td style='text-align: center;'>-0.54*</td><td style='text-align: center;'>-0.21*</td><td style='text-align: center;'>-0.1*</td><td style='text-align: center;'>-0.04</td></tr></table>\n\n<div style=\"text-align: center;\">Table 8: Changes in English summarization evaluated with different metrics. For most of the models and simplifiers, the scores decrease (red boxes). Only a few combinations show improved performance (blue boxes). * statistically significant change  $ (p < 0.05) $ , significant changes have a darker color,  $ \\dagger $  evaluated and compared only on the fixed subset</div>\n"
      }
    ]
  ]
}